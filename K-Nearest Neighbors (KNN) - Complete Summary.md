# K-Nearest Neighbors (KNN) - Complete Summary

## ЁЯУЪ Core Concept

**KNN ржХрзА?**
- Instance-based, lazy learning algorithm
- ржорзВрж▓ ржирзАрждрж┐: "рждрзБржорж┐ рждрзЛржорж╛рж░ ржмржирзНржзрзБржжрзЗрж░ ржорждрзЛ" - Similar things exist close together
- ржирждрзБржи data point classify/predict ржХрж░рждрзЗ K ржЯрж╛ nearest neighbors ржПрж░ vote/average ржирзЗржпрж╝

**Key Philosophy:**
- ржХрзЛржирзЛ model/formula рж╢рж┐ржЦрзЗ ржирж╛
- рж╕ржм training data ржоржирзЗ рж░рж╛ржЦрзЗ (memorize)
- Prediction ржП рж╕ржм data check ржХрж░рзЗ distance ржжрж┐ржпрж╝рзЗ

---

## ЁЯОп KNN ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?

### Classification ржПрж░ ржЬржирзНржп:
1. ржирждрзБржи point ржерзЗржХрзЗ рж╕ржм training points ржПрж░ **distance** рж╣рж┐рж╕рж╛ржм ржХрж░рзЛ
2. рж╕ржмржЪрзЗржпрж╝рзЗ ржХрж╛ржЫрзЗрж░ **K ржЯрж╛ neighbors** ржмрзЗржЫрзЗ ржирж╛ржУ
3. рж╕рзЗржЗ K ржЬржирзЗрж░ ржоржзрзНржпрзЗ **majority vote** ржжрзЗржЦрзЛ
4. ржпрзЗ class ржмрзЗрж╢рж┐ vote ржкрж╛ржмрзЗ, рж╕рзЗржЯрж╛ржЗ prediction

**Example:** Email spam detection - K=5 neighbors ржПрж░ ржоржзрзНржпрзЗ 4ржЯрж╛ spam рж╣рж▓рзЗ тЖТ Spam!

### Regression ржПрж░ ржЬржирзНржп:
1. ржПржХржЗ process (distance тЖТ K neighbors)
2. Vote ржирж╛ ржирж┐ржпрж╝рзЗ рждрж╛ржжрзЗрж░ values ржПрж░ **average** ржирж╛ржУ
3. рж╕рзЗржЗ average ржЗ prediction

**Example:** House price - K=5 neighbors ржПрж░ price: 50, 52, 48, 60, 55 тЖТ Average = 53 lakh

---

## ЁЯФС Key Parameters

### 1. K (n_neighbors) - рж╕ржмржЪрзЗржпрж╝рзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг!

**Small K (K=1, K=3):**
- тЬЕ Local patterns ржзрж░рзЗ ржнрж╛рж▓рзЛ
- тЭМ Noise/outliers ржП sensitive
- тЭМ Overfitting рж╣ржпрж╝ (training perfect, test ржЦрж╛рж░рж╛ржк)
- **High Variance, Low Bias**

**Large K (K=50, K=100):**
- тЬЕ Noise ржерзЗржХрзЗ stable
- тЭМ Important local patterns ржорж┐рж╕ ржХрж░рзЗ
- тЭМ Underfitting рж╣ржпрж╝ (рж╕ржм ржЬрж╛ржпрж╝ржЧрж╛ржпрж╝ average ржжрзЗржпрж╝)
- **Low Variance, High Bias**

**Best K:**
- Medium value (рж╕рж╛ржзрж╛рж░ржгржд K=5 to K=15)
- Validation/Cross-validation ржжрж┐ржпрж╝рзЗ ржЦрзБржБржЬрждрзЗ рж╣ржпрж╝
- Rule of thumb: K тЙИ тИЪn (n = training samples)
- рж╕ржмрж╕ржоржпрж╝ **odd number** рж░рж╛ржЦрзЛ (tie avoid ржХрж░рждрзЗ)

### 2. Distance Metric

**Euclidean Distance (p=2):** [Default, ржмрзЗрж╢рж┐рж░ржнрж╛ржЧ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржнрж╛рж▓рзЛ]
- Formula: тИЪ[(xтВБ-yтВБ)┬▓ + (xтВВ-yтВВ)┬▓ + ...]
- рж╕рж░рж╛рж╕рж░рж┐ straight line distance
- Continuous features ржП ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ

**Manhattan Distance (p=1):**
- Formula: |xтВБ-yтВБ| + |xтВВ-yтВВ| + ...
- Grid ржмрж░рж╛ржмрж░ distance (L/R + U/D)
- High dimensional data ржмрж╛ outliers ржерж╛ржХрж▓рзЗ better

### 3. Weights

**Uniform:** рж╕ржм neighbors ржПрж░ vote рж╕ржорж╛ржи
**Distance-weighted:** ржХрж╛ржЫрзЗрж░ neighbor = ржмрзЗрж╢рж┐ vote (weight = 1/distance)
- Class imbalance ржерж╛ржХрж▓рзЗ weighted ржнрж╛рж▓рзЛ

---

## тЪЦя╕П Bias-Variance Tradeoff

**Bias (ржкржХрзНрж╖ржкрж╛ржд):**
- Model ржХрждржЯрж╛ рж╕рж░рж▓/oversimplified?
- High Bias = pattern ржзрж░рждрзЗ ржкрж╛рж░рзЗ ржирж╛

**Variance (ржЕрж╕рзНржерж┐рж░рждрж╛):**
- Model ржХрждржЯрж╛ sensitive/unstable?
- High Variance = noise ржХрзЗржУ pattern ржоржирзЗ ржХрж░рзЗ

**KNN рждрзЗ:**
```
K тЖУ (ржХржорж╛рж▓рзЗ) тЖТ Bias тЖУ, Variance тЖС тЖТ Overfitting
K тЖС (ржмрж╛ржбрж╝рж╛рж▓рзЗ) тЖТ Bias тЖС, Variance тЖУ тЖТ Underfitting
```

**Goal:** Medium K ржжрж┐ржпрж╝рзЗ balance ржХрж░рзЛ!

---

## ЁЯФ┤ Overfitting vs Underfitting

**Overfitting (K ржЫрзЛржЯ, ржпрзЗржоржи K=1):**
- Training data perfect ржоржирзЗ рж░рж╛ржЦрзЗ
- Test data рждрзЗ ржнрзБрж▓ ржХрж░рзЗ
- Noise ржУ ржоржирзЗ рж░рж╛ржЦрзЗ
- Example: ржкрж░рзАржХрзНрж╖рж╛рж░ ржкрзНрж░рж╢рзНржи рж╣рзБржмрж╣рзБ ржорзБржЦрж╕рзНрже ржХрж░рж╛

**Underfitting (K ржмржбрж╝, ржпрзЗржоржи K=100):**
- ржХрзЛржирзЛ pattern ржЗ ржзрж░рзЗ ржирж╛
- рж╕ржм ржЬрж╛ржпрж╝ржЧрж╛ржпрж╝ average ржжрзЗржпрж╝
- Training ржУ test ржжрзБржЯрзЛрждрзЗржЗ ржЦрж╛рж░рж╛ржк
- Example: ржХрж┐ржЫрзБржЗ ржирж╛ ржкржбрж╝рзЗ ржкрж░рзАржХрзНрж╖рж╛ ржжрзЗржУржпрж╝рж╛

**Good Fit (K medium, ржпрзЗржоржи K=5-10):**
- Pattern ржзрж░рждрзЗ ржкрж╛рж░рзЗ
- Noise ignore ржХрж░рзЗ
- Test data рждрзЗржУ ржнрж╛рж▓рзЛ ржХрж░рзЗ

---

## ЁЯЪА Why KNN is "Lazy Learner"?

**Eager Learners (ржЕржирзНржп ML models):**
- Training: рж╢рж┐ржЦрзЗ formula/model ржмрж╛ржирж╛ржпрж╝ (рж╕ржоржпрж╝ рж▓рж╛ржЧрзЗ)
- Prediction: formula apply ржХрж░рзЗ (ржжрзНрж░рзБржд)

**Lazy Learner (KNN):**
- Training: ржХрж┐ржЫрзБ рж╢рж┐ржЦрзЗ ржирж╛, рж╢рзБржзрзБ data store ржХрж░рзЗ (instant!)
- Prediction: рж╕ржм data ржШрзЗржБржЯрзЗ ржжрзЗржЦрзЗ (slow!)

**Analogy:**
- Eager = ржкрж░рзАржХрзНрж╖рж╛рж░ ржЖржЧрзЗ ржкржбрж╝рзЗ notes ржмрж╛ржирж╛рж▓рзЛ, ржкрж░рзАржХрзНрж╖рж╛ржпрж╝ ржжрзНрж░рзБржд рж▓рж┐ржЦрж▓рзЛ
- Lazy = ржХрж┐ржЫрзБ ржирж╛ ржкржбрж╝рзЗ ржмржЗ ржирж┐ржпрж╝рзЗ ржПрж▓рзЛ, ржкрж░рзАржХрзНрж╖рж╛ржпрж╝ ржмржЗ ржЦрзБржБржЬрзЗ ржЙрждрзНрждрж░ рж▓рж┐ржЦрж▓рзЛ

---

## тЬЕ When KNN Excels

- тЬУ Small to medium datasets (< 10,000 samples)
- тЬУ Low dimensional data (< 20 features)
- тЬУ Non-linear decision boundaries
- тЬУ No training time available (instant model)
- тЬУ Complex/irregular patterns
- тЬУ Need interpretability ("ржПржЗ 5 ржЬржиржУ ржПржЯрж╛ ржХрж░рзЗржЫрзЗ, рждрж╛ржЗ...")

---

## тЭМ When to Avoid KNN

- тЬЧ Large datasets (millions) - prediction ржЕржирзЗржХ slow
- тЬЧ High dimensional data (>50 features) - curse of dimensionality
- тЬЧ Real-time/fast prediction needed - ржкрзНрж░рждрж┐ржмрж╛рж░ рж╕ржм data check
- тЬЧ Class imbalance (without proper handling)
- тЬЧ Many irrelevant/noisy features
- тЬЧ Limited memory - ржкрзБрж░рзЛ data store ржХрж░рждрзЗ рж╣ржпрж╝

---

## ЁЯЫая╕П Feature Scaling - Mandatory!

**ржХрзЗржи ржЬрж░рзБрж░рж┐?**
```
Without Scaling:
Feature 1 (Age): 20-80
Feature 2 (Income): 20000-100000

Distance = тИЪ[(5)┬▓ + (50000)┬▓] 
тЖТ Income dominates! Age ржПрж░ effect ржирзЗржЗ

With Scaling (StandardScaler):
Feature 1: -1.5 to +1.5
Feature 2: -1.5 to +1.5

Distance = тИЪ[(0.5)┬▓ + (0.5)┬▓]
тЖТ Both equal importance! тЬУ
```

**StandardScaler Formula:**
```
scaled = (value - mean) / std_deviation
```

**Impact:** Scaling ржирж╛ ржХрж░рж▓рзЗ 20-30% accuracy ржХржорзЗ ржпрзЗрждрзЗ ржкрж╛рж░рзЗ!

---

## ЁЯУК Implementation Steps

### 1. Data Preparation
```python
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

# Load data
data = load_wine()
X, y = data.data, data.target

# Split (stratify for balanced classes)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)
```

### 2. Build Pipeline (Scaling + KNN)
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

model = Pipeline([
    ("scaler", StandardScaler()),  # Mandatory!
    ("knn", KNeighborsClassifier(n_neighbors=5))
])
```

### 3. Train and Predict
```python
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### 4. Evaluate
```python
from sklearn.metrics import accuracy_score, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
```

### 5. Tune K (Critical!)
```python
k_values = range(1, 31)
accs = []

for k in k_values:
    model_k = Pipeline([
        ("scaler", StandardScaler()),
        ("knn", KNeighborsClassifier(n_neighbors=k))
    ])
    model_k.fit(X_train, y_train)
    pred = model_k.predict(X_test)
    accs.append(accuracy_score(y_test, pred))

best_k = k_values[np.argmax(accs)]
```

### 6. Compare Settings
```python
settings = [
    ("Euclidean uniform", KNeighborsClassifier(n_neighbors=best_k, p=2, weights="uniform")),
    ("Manhattan uniform", KNeighborsClassifier(n_neighbors=best_k, p=1, weights="uniform")),
    ("Euclidean weighted", KNeighborsClassifier(n_neighbors=best_k, p=2, weights="distance"))
]
```

---

## ЁЯОУ Practical Guidelines

1. **рж╕ржмрж╕ржоржпрж╝ K odd рж░рж╛ржЦрзЛ** (tie avoid ржХрж░рждрзЗ)
2. **K range:** 3 тЙд K тЙд тИЪn
3. **Start with K = тИЪn**, рждрж╛рж░ржкрж░ tune ржХрж░рзЛ
4. **Feature scaling ржХрж░рждрзЗржЗ рж╣ржмрзЗ** (StandardScaler)
5. **Cross-validation ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЛ** best K ржЦрзБржБржЬрждрзЗ
6. **Class imbalance ржерж╛ржХрж▓рзЗ** weighted voting ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЛ
7. **Avoid K=1** (overfitting) ржПржмржВ **K=n** (underfitting)

---

## ЁЯУИ Model Selection Strategy
```
1. Quick baseline тЖТ KNN (K=5, scaled)
2. K tuning тЖТ Try 1 to 30, plot accuracy
3. Best K ржжрж┐ржпрж╝рзЗ тЖТ Compare distance metrics
4. Final model тЖТ Euclidean + best K + scaling
5. Evaluate тЖТ Confusion matrix, classification report
6. Compare тЖТ With/without scaling difference
```

---

## ЁЯФД Validation Process

**Train-Validation-Test Split:**
```
Total Data (100%)
    тЖУ
Training (70%) + Test (30%)
    тЖУ
Training Split:
  - Training Set (80%)
  - Validation Set (20%)
```

**Cross-Validation (Better):**
- Data ржХрзЗ 5 ржнрж╛ржЧрзЗ ржнрж╛ржЧ ржХрж░рзЛ
- ржкрзНрж░рждрж┐ржмрж╛рж░ 1 ржнрж╛ржЧ validation, ржмрж╛ржХрж┐ 4 ржнрж╛ржЧ training
- 5 ржмрж╛рж░ repeat ржХрж░рзЛ
- Average accuracy ржирж╛ржУ

---

## ЁЯЖЪ KNN vs Model-Based Learning

| Feature | KNN | Model-Based (e.g., Linear Regression) |
|---------|-----|--------------------------------------|
| Training | Instant (data store) | Takes time (learn parameters) |
| Prediction | Slow (scan all data) | Fast (apply formula) |
| Memory | High (all data) | Low (few parameters) |
| Flexibility | Very high | Limited (assumptions) |
| Interpretability | Medium | High (see coefficients) |

**Progression:**
```
KNN (simple) 
  тЖТ Logistic Regression (faster prediction)
  тЖТ Decision Trees (interpretable rules)
  тЖТ Random Forest, Neural Networks (complex)
```

---

## ЁЯТб Key Takeaways

1. **KNN = Distance-based lazy learner** - memorizes, doesn't learn
2. **K controls complexity** - small K = complex, large K = simple
3. **Bias-Variance tradeoff** - ржПржХржЯрж╛ ржХржорж╛рж▓рзЗ ржЕржирзНржпржЯрж╛ ржмрж╛ржбрж╝рзЗ
4. **Scaling is mandatory** - ржирж╛рж╣рж▓рзЗ 20-30% accuracy ржХржорзЗ
5. **Best K through validation** - experiment ржХрж░рзЗ ржЦрзБржБржЬрждрзЗ рж╣ржпрж╝
6. **Good for:** Small data, non-linear patterns, quick baseline
7. **Bad for:** Large data, high dimensions, real-time prediction
8. **Core lessons:** Distance metrics, overfitting/underfitting, hyperparameter tuning

---

## ЁЯОп Real-World Example (Breast Cancer)

**Dataset:** 569 patients, 30 features, 2 classes (malignant/benign)

**Results:**
- Best K = 5
- Accuracy with scaling = 97.9%
- Accuracy without scaling = 93.0%
- Improvement = +4.9%

**Insights:**
- Euclidean distance worked best
- 3 False Negatives (missed cancer cases - critical in medical context!)
- Scaling improved performance significantly
- Model ready but needs careful validation for real medical use

---
